## Лаба 0. **Поднять свой кластер и отфильтровать логи из HDFS в HBase**

​        <img src="http://www.ditoweb.com/wp-content/uploads/2014/11/googlecloudplatform-logo-1-1024x265.png" width="370px" align="center">  <img src="https://hbase.apache.org/images/hbase_logo_with_orca_large.png" width="300px" align="center">  

Ну что, давайте подготовим кластер для дальнейшей работы и немного разомнемся на задаче, связанной с HDFS и HBase. Кто-то это сделает впервые, а кто-то просто освежит свои воспоминания.

### 1. Развертывание виртуальных машин

Поскольку главная задача любого дата инженера — это построение пайплайна обработки и перемещения данных (а этот процесс требует конфигурирования разных инструментов), то возникает потребность в том, чтобы у каждого участника программы был свой собственный кластер.

Проанализировав различные облачные платформы, мы пришли к выводу, что наилучшим вариантом на текущий момент для нас будет <a href="https://cloud.google.com">Google Cloud Platform</a>. Там при регистрации дают $300, которые можно потратить в течение года на любые сервисы (не пугайтесь, что для юр. лица и не пугайтесь, что потребуют данные карточки, списывать с нею без разрешения они не будут, превышение лимита невозможно). Этого должно хватить на всю программу при аккуратном использовании. В частности, нужно **выключать машины, когда они не используются**.

После регистрации вам предложат создать свой новый проект и назвать его. Название может быть любым. Можете быть оригинальными.

Зайдите в раздел `Metadata` (карточка Compute Engine), далее во вкладку `SSH Keys`. Сюда вы можете вставить значение вашего pub-ключа, чтобы потом зайти на любую машину со своим private-key. <a href="https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys">Здесь на GPC</a> подробно написано, как можно создать свой ключ с нуля на MacOS и Windows. В итоге после добавления ключа у вас должно получиться что-то такое:

<img src="http://data.newprolab.com/public-newprolab-com/de_lab01_pub.png">

Далее вам нужно будет попасть в раздел `Compute Engine`, после чего в подраздел `VM Instances`, где мы и создадим 4 виртуальных машины для нашего кластера.

Тип машины: `4 vCPUs, 15 GB memory`. Операционная система: `Ubuntu 16.04 и диск на 30 ГБ`.

Создайте 2 таких машины в регионе `europe-west1-b` и еще 2 в регионе `europe-west2-b`. К сожалению, у GCP есть квоты на количество CPU в одном регионе, которые можно изменить только, если у вас не free-аккаунт. На них вы автоматом сможете зайти с тем ключом, который добавили ранее. 

Следующее действие — зарезервировать статический IP для вашей мастер-ноды. Это потребует примерно 10 долларов из тех $300 за всё время программы. Поскольку придется включать и выключать машины, то для дальнейшего удобства это будет необходимо. Для этого перейдите из раздела `Compute Engine` в раздел `VPC Network`. Далее вкладка `External IP addresses`. Там в списке ваших серверов найдите тот, который вы собираетесь сделать мастер-нодой, и нажмите на `Ephemeral`. Там можно сделать резерв этого IP в качестве статического. 

**Важно! Скиньте этот IP вашему координатору**.

И еще один момент — обязательно откройте порты на своем сервере. Иначе и вы не сможете заходить на разные UI через браузер, и мы не сможем подконнектиться к вам и проверить лабораторную.

### 2. Установка Hortonworks HDP

Подробный мануал о том, как устанавливать HDP через Ambari изложен <a href="HDP.md">здесь</a>. Решили вынести это в отдельный документ, потому что часть из вас это может выполнить полузакрытыми глазами.

### 3. Получение и загрузка данных в HDFS

На [странице в личном кабинете](http://lk.newprolab.com/lab/de_lab_00) у вас указано название папки, из которой нужно скачать данные. Данные хранятся здесь: `data.nplcloud.com/data-newprolab-com/`.

В директориях `facetz_2015_**_**` расположены файлы вида `part-NNNNN`, где `NNNNN` - порядковый номер файла.

Каждый файл представляет собой коллекцию записей, по записи на строчку, где каждая строка представляет собой кортеж `(UID, timestamp, URL)`.

- `UID` — уникальный идентификатор пользователя, представленный натуральным числом, записанным в десятичной форме. 
- `timestamp` — отметка времени, записанная в форме UNIX timestamp, записана в виде десятичной дроби. 
- `URL` — экранированный URL, представлен в виде строки. 
- Записи разделены символом табуляции `\t`.

Пример:

`26153949061	1422751272.768	http%3A%2F%2Frzd.ru%2F`

К себе на сервер можно скачать данные, написав скрипт, который будет выкачивать файлы по одному из вашей папки. Пример выкачки одного файла:

```$ wget http://data.nplcloud.com/data-newprolab-com/facetz_2015_02_01/part-00018```

Для того, чтобы узнать наименование всех файлов в вашей директории, воспользуйтесь данной ссылок 

`http://data.nplcloud.com/data-newprolab-com/links.list`

Залейте эти данные себе на HDFS. Для этого перейдите под пользователя hdfs:

```$ sudo su hdfs```

Создайте в корне папку `labs`.

```$ hdfs dfs -mkdir /labs```

Команда `put` вам поможет потом загрузить эти данные в папку `labs` на HDFS.

### 4. Задача

Отлично, теперь у вас есть данные. Они лежат на HDFS. Теперь, что с ними нужно сделать.

Представьте, что вы разрабатываете новую рекламную кампанию и хотите среди всех пользователей выбрать только тех, кто соответствует определенной категории интересов. UID пользователя в данных генерировался не случайно, а в него закладывалась информация о категории интереса пользователя.

Для решения задачи вам нужно загрузить данные из кластерного HDFS в HBase при помощи MapReduce, осуществив предварительную фильтрацию данных.

Для выполнения работы вам следует **взять все файлы** из директории.

Замечание: неправильно сформированные строки следует игнорировать.
Неправильные строки — строки, в которых нет хотя бы одного из трёх элементов, в `URL` или в `UID` указан прочерк, или, вообще, неверный формат. URL должны начинаться с http.

Вам необходимо оставить только те строки, для которых выполняется индивидуальное условие: `UID mod 256 == N`, где N берётся из личного кабинета. URL не нужно нормализовывать, не нужно вообще никак преобразовывать.

### 4. Требования к результату

1. Название таблицы, которую вам необходимо заполнить в HBase, должно совпадать с логином ЛК.
2. Для каждой записи в таблице должно храниться 4096 версий.
3. Содержимое таблицы должно представлять собой rowkey=`uid` и column=`data:url`. Отметка времени в базе должна совпадать с отметкой в файле, а не с моментом, когда запись попала в базу. 
4. Отметку надо умножить на 1000 и привести к int: `int(ts * 1000)`.
5. Вам нужно поднять [thrift-интерфейс](https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_command-line-installation/content/ref-2a6efe32-d0e1-4e84-9068-4361b8c36dc8.1.html) на порту 9090 на мастер-ноде.
6. Проверка осуществляется автоматическим скриптом из личного кабинета.

### 6. Подсказки

1. Написать MapReduce-job — значит написать файлы `mapper.py` и `reducer.py`, а затем указать путь к ним в строке запуска Hadoop Streaming. В нашем случае можно обойтись только файлом с маппером, потому что задача фильтрации относится к map-only джобам.

2. Шаблон маппера можно взять [отсюда](https://docs.google.com/document/d/14MydN5DyrDlGHlrT-Vh7Og72jS08r404wTYxmadU6ts/edit?usp=sharing).

3. Для загрузки данных в python предлагаем воспользоваться библиотекой [Happybase](http://happybase.readthedocs.org/en/latest/).

   Подключаться с помощью `happybase` нужно к вашему хосту. Пример создания соединения в python:

   ```
   import happybase
   connection = happybase.Connection('node3.nplcloud.com')
   ```

4. Сначала напишите программу которая принимает на stdin файл и складывает его в вашу базу в HBase. Если локально отработает — запускайте на hadoop.

5. В первой строчке исполняемого python-файла нужно прописать путь к интерпретатору `#!/usr/bin/env python3`, тогда в самой команде запуска `hadoop-streaming` не придётся прописывать, чем открывать файл.

6. Если у вас MapReduce job выдаёт ошибки, то для дебага воспользуйтесь http-ссылкой на статус джобы. Её можно найти в логе инициализации джобы, который выводится вам в консоль:

   ```
   17/03/10 15:14:42 INFO impl.YarnClientImpl: Submitted application application_1488372559028_0002
   17/03/10 15:14:42 INFO mapreduce.Job: The url to track the job: http://node1.newprolab.com:8088/proxy/application_1488372559028_0002/
   ```

   В веб-интерфейсе нажмите на ссылку "logs", по которой вы получите достаточно логов (в том числе, stderr-вывод python-скрипта, в котором у вас наверняка и есть ошибка):

   ```
   Log Type: stderr
   Log Upload Time: Fri Mar 10 15:15:45 +0300 2017
   Log Length: 498
   Traceback (most recent call last):
     File "/disk1/hadoop/yarn/local/usercache/hdfs/appcache/application_1488372559028_0002/container_e01_1488372559028_0002_01_000034/./m.py", line 4, in <module>
       import happybase
   ImportError: No module named happybase
   ```

   У HBase есть веб-интерфейс. Обычно на порту 16010. Здесь вы, например, можете посмотреть, какие таблицы созданы и есть ли среди них ваша.
